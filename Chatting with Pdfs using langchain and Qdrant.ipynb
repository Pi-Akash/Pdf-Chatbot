{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11292910-8ca1-4fe8-91d7-f767ad8c8396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d33f5d-f809-4038-8fd3-9dccc8b2c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf file location\n",
    "localpath = \"C:/Drive D/Documents/Why should I trust you.pdf\"\n",
    "\n",
    "# read the file from the location\n",
    "try:\n",
    "    pdf_loader = UnstructuredPDFLoader(file_path = localpath)\n",
    "    pdf_data = pdf_loader.load()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "722a76d7-fa11-481f-9819-ab816010cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573809ce-266b-4bc9-835e-4930829592f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 2000, chunk_overlap = 20)\n",
    "chunks = text_splitter.split_documents(pdf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9d34ee9-8de9-42e9-a40f-3860d8230ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:06<00:00,  6.54s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 38/38 [03:46<00:00,  5.96s/it]\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:6333/\"\n",
    "qdrant = Qdrant.from_documents(\n",
    "    chunks,\n",
    "    OllamaEmbeddings(model = \"nomic-embed-text\", show_progress = True),\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"local-qdrant-rag\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc793e6-3e78-42ef-9007-8d0e606775fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78553d90-176e-4c40-9b71-530194a156b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm from google\n",
    "local_model = \"gemma:2b\"\n",
    "llm = ChatOllama(model = local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbcf89cd-8d04-4df0-8ac6-7a3e2c4920e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77ae19ac-b60c-44c1-950a-a0365c74b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    qdrant.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc1f6401-7e02-404e-ac5d-79d5801164b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26407a5e-0f98-4ced-962d-020b274f3878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what this document is about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This document is about the purpose and techniques of explaining and selecting instances for a machine learning model, particularly when dealing with trust-related tasks.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fad9c-a93b-4721-b2a6-c08b2a17da79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
